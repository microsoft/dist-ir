{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiac-cricket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8f043bb0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_before(n, w_1, w_2, x):  # (h, 3h), (h, h), (s, b, h)\n",
    "    s, b, h = x.shape\n",
    "    x1 = np.matmul(x, w_1)  # (s, b, 3h)\n",
    "\n",
    "    x2 = np.reshape(x1, (s, b, n, 3 * h // n))\n",
    "    q, k, v = np.split(x2, 3, axis=3)  # (s, b, n, h/n)\n",
    "\n",
    "    q = np.reshape(q, (s, b * n, h // n))\n",
    "    q = np.transpose(q, (1, 0, 2))  # (bn, s, h/n)\n",
    "    k = np.reshape(k, (s, b * n, h // n))\n",
    "    k = np.transpose(k, (1, 2, 0))  # (bn, h/n, s)\n",
    "    v = np.reshape(v, (s, b * n, h // n))\n",
    "    v = np.transpose(v, (1, 0, 2))  # (bn, s, h/n)\n",
    "\n",
    "    y1 = np.matmul(q, k)  # (bn, s, s)\n",
    "\n",
    "    # Ignoring a scale mask, softmax, and dropout here\n",
    "\n",
    "    y2 = np.matmul(y1, v)  # (bn, s, h/n)\n",
    "    # No idea why megatron goes through these layout changes:\n",
    "    y2 = np.reshape(y2, (b, n, s, h // n))\n",
    "    y2 = np.transpose(y2, (2, 0, 1, 3))  # (s, b, n, h/n)\n",
    "    y2 = np.reshape(y2, (s, b, h))\n",
    "\n",
    "    # RowParallelLinear\n",
    "    z = np.matmul(y2, w_2)  # (s, b, h)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elementary-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_pytorch():\n",
    "    s = 3  # sequence length\n",
    "    b = 4  # batch size\n",
    "    n = 2  # num attention heads\n",
    "    h = 6  # hidden size\n",
    "    p = 2  # num partitions\n",
    "\n",
    "    def attention(x, w_1, w_2):\n",
    "        x1 = torch.matmul(x, w_1)  # (s, b, 3h)\n",
    "\n",
    "        x2 = torch.reshape(x1, (s, b, n, 3 * h // n))\n",
    "        q, k, v = torch.split(x2, 3, dim=3)  # (s, b, n, h/n)\n",
    "\n",
    "        q = torch.reshape(q, (s, b * n, h // n))\n",
    "        q = q.permute(1, 0, 2)  # (bn, s, h/n)\n",
    "        k = torch.reshape(k, (s, b * n, h // n))\n",
    "        k = k.permute(1, 2, 0)  # (bn, h/n, s)\n",
    "        v = torch.reshape(v, (s, b * n, h // n))\n",
    "        v = v.permute(1, 0, 2)  # (bn, s, h/n)\n",
    "\n",
    "        y1 = torch.matmul(q, k)  # (bn, s, s)\n",
    "\n",
    "        # Ignoring a scale mask, softmax, and dropout here\n",
    "\n",
    "        y2 = torch.matmul(y1, v)  # (bn, s, h/n)\n",
    "        # No idea why megatron goes through these layout changes:\n",
    "        y2 = torch.reshape(y2, (b, n, s, h // n))\n",
    "        y2 = y2.permute(2, 0, 1, 3)  # (s, b, n, h/n)\n",
    "        y2 = torch.reshape(y2, (s, b, h))\n",
    "\n",
    "        # RowParallelLinear\n",
    "        z = torch.matmul(y2, w_2)  # (s, b, h)\n",
    "\n",
    "        np.testing.assert_array_almost_equal(\n",
    "            z.detach().numpy(),\n",
    "            self_attention_before(\n",
    "                n, w_1.detach().numpy(), w_2.detach().numpy(), x.numpy()\n",
    "            ),\n",
    "            decimal=4,\n",
    "        )\n",
    "        \n",
    "    x = torch.randn((s, b, h))  # model input/output of previous layer\n",
    "    w_1 = torch.autograd.Variable(torch.randn((h, 3 * h)), requires_grad=True)\n",
    "    w_2 = torch.autograd.Variable(torch.randn(h, h), requires_grad=True)\n",
    "    attention(x, w_1, w_2)\n",
    "#     traced_attention = torch.jit.trace(attention, (x, w_1, w_2))\n",
    "#     torch.jit.save(traced_attention, \"attention.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prospective-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_pytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-wheat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
