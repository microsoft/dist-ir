{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loaded-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, OrderedDict\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import dist_ir\n",
    "from dist_ir.importer import import_from_onnx, parse_tensor_from_file\n",
    "from dist_ir.ir import FunctionMaker, cpprint, pformat, Device, Topology, Value\n",
    "from dist_ir.executor import infer_types, SequentialExecutor, Simulator\n",
    "from dist_ir.executor.cost_model import CostModel\n",
    "from dist_ir.ir.type import Bool, Float, Int64, Tensor\n",
    "from dist_ir.transforms import (\n",
    "    parallel_transform_3d,\n",
    "    hybrid_transform_unrolled,\n",
    "    PipeDreamScheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continuing-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "DGX_BANDWIDTH_GBPS = 200 * 8.0\n",
    "device_speeds = {\"gpu\": 1.0e13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hairy-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, device):\n",
    "    function = FunctionMaker(name=\"mlp\")\n",
    "    x = function.add_input_value(\n",
    "        \"x\",\n",
    "        Tensor(\n",
    "            dtype=Float(), shape=(batch_size, input_dim), device=device\n",
    "        ),\n",
    "    )\n",
    "    z = function.add_input_value(\n",
    "        \"z\",\n",
    "        Tensor(\n",
    "            dtype=Float(), shape=(batch_size, output_dim), device=device\n",
    "        ),\n",
    "    )\n",
    "    weights = []\n",
    "    input_dim = input_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        w = function.add_input_value(\n",
    "            f\"w{chr(ord('A')+i)}\",\n",
    "            Tensor(dtype=Float(), shape=(input_dim, hidden_dim), device=device),\n",
    "        )\n",
    "        input_dim = hidden_dim\n",
    "        weights.append(w)\n",
    "    w = function.add_input_value(\n",
    "        f\"w{chr(ord('A')+i+1)}\",\n",
    "        Tensor(dtype=Float(), shape=(hidden_dim, output_dim), device=device),\n",
    "    )\n",
    "    weights.append(w)\n",
    "\n",
    "    a = x\n",
    "    for i, weight in enumerate(weights):\n",
    "        y = function.add_op(\"MatMul\", inputs=[a, weight], output_names=[f\"y{i}\"])\n",
    "        a = function.add_op(\"Relu\", inputs=[y], output_names=[f\"a{i}\"])\n",
    "\n",
    "    l = function.add_op(\n",
    "        \"Loss\", inputs=[a, z], attributes={\"N\": batch_size}, output_names=[\"l\"]\n",
    "    )\n",
    "    dl = function.add_op(\n",
    "        \"LossGrad\",\n",
    "        inputs=[a, z],\n",
    "        attributes={\"N\": batch_size},\n",
    "        output_names=[\"dl\"],\n",
    "    )\n",
    "\n",
    "    dy = dl\n",
    "    for i, weight in enumerate(weights[::-1]):\n",
    "        i = len(weights) - i - 1\n",
    "        da = function.add_op(\n",
    "            \"ReluGrad\",\n",
    "            inputs=[function.ops[2 * i + 1].inputs[0], dy],\n",
    "            output_names=[f\"da{i}\"],\n",
    "        )\n",
    "        dy, dw = function.add_op(\n",
    "            \"MatMulGrad\",\n",
    "            inputs=[function.ops[2 * i].inputs[0], weights[i], da],\n",
    "            output_names=[f\"dy{i}\", f\"dw{chr(ord('A')+i)}\"],\n",
    "        )\n",
    "    return function.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serious-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_devices_to_topology(topology, num_devices):\n",
    "    for i in range(num_devices):\n",
    "        topology.add_device(\"gpu\")\n",
    "    devices = topology.devices\n",
    "    for i in range(1, len(devices)):\n",
    "        topology.set_bandwidth(devices[0], devices[i], float(\"inf\"))\n",
    "        for j in range(i, len(devices)):\n",
    "            topology.set_bandwidth(devices[i], devices[j], DGX_BANDWIDTH_GBPS)\n",
    "    return topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "premium-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_degrees(n):\n",
    "    all_degrees = []\n",
    "    d = 1\n",
    "    h = 1\n",
    "    p = 1\n",
    "    while d <= n:\n",
    "        h = 1\n",
    "        p = 1\n",
    "        if d * h * p == n:\n",
    "            all_degrees.append((d, h, p))\n",
    "            break\n",
    "        while h <= n:\n",
    "            p = 1\n",
    "            if d * h * p == n:\n",
    "                all_degrees.append((d, h, p))\n",
    "                break\n",
    "            while p <= n:\n",
    "                if d * h * p == n:\n",
    "                    all_degrees.append((d, h, p))\n",
    "                    break\n",
    "                p *= 2\n",
    "            h *= 2\n",
    "        d *= 2\n",
    "    return all_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential running time: 1.127743488 ms\n"
     ]
    }
   ],
   "source": [
    "input_dim = 512\n",
    "hidden_dim = 512\n",
    "output_dim = 1\n",
    "all_cluster_sizes = [64]#[64, 128, 512, 1024]\n",
    "all_num_hidden_layers = [8]#[4, 8, 16, 32]\n",
    "all_batch_sizes = [1024]#[512, 1024, 2048, 4096, 8192]\n",
    "microbatch_size = 256\n",
    "results = []\n",
    "for num_hidden_layers in all_num_hidden_layers:\n",
    "    for batch_size in all_batch_sizes:\n",
    "        num_microbatches = batch_size // microbatch_size\n",
    "        topology = Topology(device_speeds)\n",
    "        d0 = topology.add_device(\"gpu\")\n",
    "        function = mlp(batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, d0)\n",
    "        function = infer_types(function, function.inputs)\n",
    "        simulator = Simulator(CostModel(topology))\n",
    "        simulation = simulator.interpret(function, (v.type for v in function.inputs))\n",
    "        sequential_running_time = max(\n",
    "            [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "        )\n",
    "        print(f\"Sequential running time: {sequential_running_time * 1e3} ms\")\n",
    "        for i, cluster_size in enumerate(all_cluster_sizes):\n",
    "            if i == 0:\n",
    "                add_devices_to_topology(topology, cluster_size)\n",
    "            else:\n",
    "                add_devices_to_topology(\n",
    "                    topology, all_cluster_sizes[i] - all_cluster_sizes[i - 1]\n",
    "                )\n",
    "            all_degrees = get_all_degrees(cluster_size)\n",
    "            for (dp_degree, hp_degree, pp_degree) in all_degrees:\n",
    "                if num_hidden_layers % pp_degree != 0:\n",
    "                    continue\n",
    "                transformed_function = parallel_transform_3d(\n",
    "                    function,\n",
    "                    dp_degree,\n",
    "                    hp_degree,\n",
    "                    pp_degree,\n",
    "                    topology.devices,\n",
    "                    num_microbatches,\n",
    "                )\n",
    "                transformed_function = infer_types(\n",
    "                    transformed_function, transformed_function.inputs\n",
    "                )\n",
    "                simulation = simulator.interpret(\n",
    "                    transformed_function, (v.type for v in transformed_function.inputs)\n",
    "                )\n",
    "                distributed_running_time = max(\n",
    "                    [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "                )\n",
    "                results.append(\n",
    "                    (\n",
    "                        num_hidden_layers,\n",
    "                        batch_size,\n",
    "                        cluster_size,\n",
    "                        dp_degree,\n",
    "                        hp_degree,\n",
    "                        pp_degree,\n",
    "                        distributed_running_time * 1e3\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-theta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-snapshot",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
