{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-tribune",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, OrderedDict\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dist_ir\n",
    "from dist_ir.importer import import_from_onnx, parse_tensor_from_file\n",
    "from dist_ir.ir import FunctionMaker, cpprint, pformat, Device, Topology, Value\n",
    "from dist_ir.executor import infer_types, SequentialExecutor, Simulator\n",
    "from dist_ir.executor.cost_model import CostModel\n",
    "from dist_ir.ir.type import Bool, Float, Int64, Tensor\n",
    "from dist_ir.transforms import (\n",
    "    parallel_transform_3d,\n",
    "    steady_state_transform,\n",
    "    PipeDreamScheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-maintenance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DGX_BANDWIDTH_GBPS = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-giant",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-stand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mlp(batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, device):\n",
    "    function = FunctionMaker(name=\"mlp\")\n",
    "    x = function.add_input_value(\n",
    "        \"x\",\n",
    "        Tensor(\n",
    "            dtype=Float(), shape=(batch_size, input_dim), device=device\n",
    "        ),\n",
    "    )\n",
    "    z = function.add_input_value(\n",
    "        \"z\",\n",
    "        Tensor(\n",
    "            dtype=Float(), shape=(batch_size, output_dim), device=device\n",
    "        ),\n",
    "    )\n",
    "    weights = []\n",
    "    input_dim = input_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        w = function.add_input_value(\n",
    "            f\"w{chr(ord('A')+i)}\",\n",
    "            Tensor(dtype=Float(), shape=(input_dim, hidden_dim), device=device),\n",
    "        )\n",
    "        input_dim = hidden_dim\n",
    "        weights.append(w)\n",
    "    w = function.add_input_value(\n",
    "        f\"w{chr(ord('A')+i+1)}\",\n",
    "        Tensor(dtype=Float(), shape=(hidden_dim, output_dim), device=device),\n",
    "    )\n",
    "    weights.append(w)\n",
    "\n",
    "    a = x\n",
    "    for i, weight in enumerate(weights):\n",
    "        y = function.add_op(\"MatMul\", inputs=[a, weight], output_names=[f\"y{i}\"])\n",
    "        a = function.add_op(\"Relu\", inputs=[y], output_names=[f\"a{i}\"])\n",
    "\n",
    "    l = function.add_op(\n",
    "        \"Loss\", inputs=[a, z], attributes={\"N\": batch_size}, output_names=[\"l\"]\n",
    "    )\n",
    "    dl = function.add_op(\n",
    "        \"LossGrad\",\n",
    "        inputs=[a, z],\n",
    "        attributes={\"N\": batch_size},\n",
    "        output_names=[\"dl\"],\n",
    "    )\n",
    "\n",
    "    dy = dl\n",
    "    for i, weight in enumerate(weights[::-1]):\n",
    "        i = len(weights) - i - 1\n",
    "        da = function.add_op(\n",
    "            \"ReluGrad\",\n",
    "            inputs=[function.ops[2 * i + 1].inputs[0], dy],\n",
    "            output_names=[f\"da{i}\"],\n",
    "        )\n",
    "        dy, dw = function.add_op(\n",
    "            \"MatMulGrad\",\n",
    "            inputs=[function.ops[2 * i].inputs[0], weights[i], da],\n",
    "            output_names=[f\"dy{i}\", f\"dw{chr(ord('A')+i)}\"],\n",
    "        )\n",
    "    return function.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-review",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_devices_to_topology(topology, num_devices):\n",
    "    for i in range(num_devices):\n",
    "        topology.add_device(\"gpu\")\n",
    "    devices = topology.devices\n",
    "    for i in range(0, len(devices)):\n",
    "        for j in range(i+1, len(devices)):\n",
    "            topology.set_bandwidth(devices[i], devices[j], DGX_BANDWIDTH_GBPS)\n",
    "    return topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-modem",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_degrees(n):\n",
    "    all_degrees = []\n",
    "    d = 1\n",
    "    h = 1\n",
    "    p = 1\n",
    "    while d <= n:\n",
    "        h = 1\n",
    "        p = 1\n",
    "        if d * h * p == n:\n",
    "            all_degrees.append((d, h, p))\n",
    "            break\n",
    "        while h <= n:\n",
    "            p = 1\n",
    "            if d * h * p == n:\n",
    "                all_degrees.append((d, h, p))\n",
    "                break\n",
    "            while p <= n:\n",
    "                if d * h * p == n:\n",
    "                    all_degrees.append((d, h, p))\n",
    "                    break\n",
    "                p *= 2\n",
    "            h *= 2\n",
    "        d *= 2\n",
    "    return all_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_communication_overhead(simulation):\n",
    "    total_time = 0.0\n",
    "    communication_time = 0.0\n",
    "    for event in simulation.trace:\n",
    "        total_time += event[\"dur\"]\n",
    "        if event[\"name\"] == \"Send\" or \"MPI\" in event[\"name\"]:\n",
    "            communication_time += event[\"dur\"]\n",
    "    return communication_time / total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-night",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-wales",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "input_dim = 8192\n",
    "hidden_dim = input_dim\n",
    "output_dim = input_dim\n",
    "all_cluster_sizes = [16]  # [64, 128, 512, 1024]\n",
    "all_num_hidden_layers = [64]  # [4, 8, 16, 32]\n",
    "all_batch_sizes = [8192]  # [512, 1024, 2048, 4096, 8192]\n",
    "results = defaultdict(lambda: defaultdict(lambda: {}))\n",
    "for num_hidden_layers in all_num_hidden_layers:\n",
    "    for batch_size in all_batch_sizes:\n",
    "        topology = Topology()\n",
    "        d0 = topology.add_device(\"gpu\")\n",
    "        function = mlp(\n",
    "            batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, d0\n",
    "        )\n",
    "        function = infer_types(function, function.inputs)\n",
    "        simulator = Simulator(CostModel(topology))\n",
    "        simulation = simulator.interpret(function, (v.type for v in function.inputs))\n",
    "        sequential_running_time = max(\n",
    "            [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "        )\n",
    "        print(f\"Sequential running time: {sequential_running_time * 1e3} ms\")\n",
    "        for i, cluster_size in enumerate(all_cluster_sizes):\n",
    "            if i == 0:\n",
    "                add_devices_to_topology(topology, cluster_size)\n",
    "            else:\n",
    "                add_devices_to_topology(\n",
    "                    topology, all_cluster_sizes[i] - all_cluster_sizes[i - 1]\n",
    "                )\n",
    "            all_degrees = get_all_degrees(cluster_size)\n",
    "            for (dp_degree, hp_degree, pp_degree) in all_degrees:\n",
    "                if num_hidden_layers % pp_degree != 0:\n",
    "                    continue\n",
    "                dp_batch_size = batch_size // dp_degree\n",
    "                if pp_degree == 1:\n",
    "                    all_num_microbatches = [1]\n",
    "                else:\n",
    "                    all_num_microbatches = [\n",
    "                        int(2 ** k)\n",
    "                        for k in range(1, int(np.floor(np.log2(dp_batch_size) / 2)))\n",
    "                    ]\n",
    "                for num_microbatches in all_num_microbatches:\n",
    "                    if pp_degree == 1:\n",
    "                        num_microbatches == 1\n",
    "                    transformed_function = parallel_transform_3d(\n",
    "                        function,\n",
    "                        dp_degree,\n",
    "                        hp_degree,\n",
    "                        pp_degree,\n",
    "                        topology.devices,\n",
    "                        num_microbatches,\n",
    "                    )\n",
    "                    transformed_function = infer_types(\n",
    "                        transformed_function, transformed_function.inputs\n",
    "                    )\n",
    "                    transformed_function, typed_input_values = steady_state_transform(\n",
    "                        transformed_function\n",
    "                    )\n",
    "                    transformed_function = infer_types(\n",
    "                        transformed_function, typed_input_values\n",
    "                    )\n",
    "                    simulation = simulator.interpret(\n",
    "                        transformed_function,\n",
    "                        (v.type for v in transformed_function.inputs),\n",
    "                    )\n",
    "                    distributed_running_time = max(\n",
    "                        [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "                    )\n",
    "                    communication_overhead = measure_communication_overhead(simulation)\n",
    "                    throughput = batch_size / distributed_running_time\n",
    "                    results[dp_degree][num_microbatches][hp_degree] = throughput\n",
    "                    print(dp_degree, hp_degree, pp_degree, num_microbatches, throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "fig, axes = plt.subplots(1, 5, figsize=(16, 3), sharex=True, sharey=True)\n",
    "plt.xscale('log')\n",
    "plt.setp(axes, xticks=[1, 2, 4, 8, 16], xticklabels=[1, 2, 4, 8, 16])\n",
    "fig.text(0.5, -.025, '# Horizontal parallel partitions (log scale)', ha='center', va='center', size=16)\n",
    "fig.text(-.025, 0.5, 'Throughput\\n(samples/second)', va='center', ha='center', rotation='vertical', size=16)\n",
    "fig.tight_layout()\n",
    "colors = {1: 'C0', 2: 'C1', 4: 'C2', 8: 'C3', 16: 'C4', 32: 'C5'}\n",
    "markers = {1: 'o', 2: 'D', 4: 'v', 8: 's', 16: '<', 32: 'x'}\n",
    "lines = []\n",
    "labels = []\n",
    "for i, dp_degree in enumerate(sorted(results.keys())):\n",
    "    line_labels_ = []\n",
    "    for j, k in enumerate(sorted(results[dp_degree].keys())):\n",
    "        x = sorted(results[dp_degree][k].keys())\n",
    "        y = [results[dp_degree][k][hp_degree] for hp_degree in x]\n",
    "        if k == 1:\n",
    "            label = \"No pipeline parallelism\"\n",
    "            l = axes[i].plot(x, y, marker=markers[k], color=colors[k], label=label)[0]\n",
    "        else:\n",
    "            label = f\"{k} microbatches\"\n",
    "            l = axes[i].plot(x, y, marker=markers[k], color=colors[k], label=f\"{k} microbatches\")[0]\n",
    "        if i == 0:\n",
    "            axes[i].set_title(f\"No data parallelism\", size=16)\n",
    "        else:\n",
    "            axes[i].set_title(f\"{dp_degree} data parallel partitions\", size=16)\n",
    "        if i == 0:\n",
    "            lines.append(l)\n",
    "            labels.append(label)\n",
    "    leg = plt.figlegend(lines, labels, loc='lower center', ncol=len(lines))\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    # Get the bounding box of the original legend.\n",
    "    bb = leg.get_bbox_to_anchor().transformed(fig.gca().transAxes.inverted())\n",
    "    yOffset = 1.4\n",
    "    bb.y0 += yOffset\n",
    "    bb.y1 += yOffset\n",
    "    leg.set_bbox_to_anchor(bb, transform = fig.gca().transAxes)\n",
    "plt.savefig(\"grid_search.pdf\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-decrease",
   "metadata": {},
   "source": [
    "## Simulation Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-process",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_dim = 64\n",
    "hidden_dim = 64\n",
    "output_dim = 64\n",
    "num_trials = 5\n",
    "topology = Topology()\n",
    "d0 = topology.add_device(\"gpu\")\n",
    "all_num_hidden_layers = [32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "all_simulation_times = defaultdict(lambda: [])\n",
    "for num_hidden_layers in all_num_hidden_layers:\n",
    "    function = mlp(\n",
    "        batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, d0\n",
    "    )\n",
    "    function = infer_types(function, function.inputs)\n",
    "    num_ops = len(function.ops)\n",
    "    for i in range(num_trials):\n",
    "        simulator = Simulator(CostModel(topology))\n",
    "        start = time.time()\n",
    "        simulation = simulator.interpret(function, (v.type for v in function.inputs))\n",
    "        assert len(simulation.trace) == num_ops\n",
    "        duration = time.time() - start\n",
    "        all_simulation_times[num_ops].append(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-honor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = sorted(all_simulation_times.keys())\n",
    "y = [1e3 * np.median(all_simulation_times[k]) for k in x]\n",
    "print(list(zip(x, y)))\n",
    "# plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"font.serif\"] = \"Times\"\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"figure.figsize\"] = (8,3)\n",
    "plt.plot([x[0]]+x[2:], [y[0]]+y[2:], marker='o')\n",
    "plt.xlabel(\"# Ops\")\n",
    "plt.ylabel(\"Milliseconds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"simulator_scaling.pdf\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-chance",
   "metadata": {},
   "source": [
    "## Isolated Parallelism Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num_devices = [2, 4, 8, 16, 32, 64, 128]\n",
    "all_batch_sizes = [512, 1024, 2048, 4096]\n",
    "input_dim = 8192\n",
    "hidden_dim = input_dim\n",
    "output_dim = hidden_dim\n",
    "num_hidden_layers = 64\n",
    "dp_results = defaultdict(list)\n",
    "for batch_size in all_batch_sizes:\n",
    "    topology = Topology()\n",
    "    d0 = topology.add_device(\"gpu\")\n",
    "    for i, num_devices in enumerate(all_num_devices):     \n",
    "        function = mlp(\n",
    "            batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, d0\n",
    "        )\n",
    "        function = infer_types(function, function.inputs)\n",
    "        if i == 0:\n",
    "            add_devices_to_topology(topology, num_devices)\n",
    "        else:\n",
    "            add_devices_to_topology(\n",
    "                topology, all_num_devices[i] - all_num_devices[i - 1]\n",
    "            )\n",
    "        assert len(topology.devices) == all_num_devices[i] + 1\n",
    "        simulator = Simulator(CostModel(topology))\n",
    "        transformed_function = parallel_transform_3d(\n",
    "            function,\n",
    "            num_devices,\n",
    "            1,\n",
    "            1,\n",
    "            topology.devices,\n",
    "            1,\n",
    "        )\n",
    "        transformed_function = infer_types(\n",
    "            transformed_function, transformed_function.inputs\n",
    "        )\n",
    "        transformed_function, typed_input_values = steady_state_transform(\n",
    "            transformed_function\n",
    "        )\n",
    "        transformed_function = infer_types(transformed_function, typed_input_values)\n",
    "        simulation = simulator.interpret(\n",
    "            transformed_function,\n",
    "            (v.type for v in transformed_function.inputs),\n",
    "        )\n",
    "        distributed_running_time = max(\n",
    "            [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "        )\n",
    "        dp_results[batch_size].append(batch_size / distributed_running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "markers = [\"o\", \"D\", \"v\", \"s\", \"<\", \"x\"]\n",
    "styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "c = np.arange(1, len(pp_results) + 3)\n",
    "norm = mpl.colors.Normalize(vmin=c.min(), vmax=c.max())\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.Blues)\n",
    "cmap.set_array([])\n",
    "plt.figure(figsize=(5, 3))\n",
    "lines = []\n",
    "labels = []\n",
    "for i, batch_size in enumerate(dp_results):\n",
    "    labels.append(f\"Batch size {batch_size}\")\n",
    "    l = plt.plot(\n",
    "        all_num_devices[:-1],\n",
    "        dp_results[batch_size][:-1],\n",
    "        marker=markers[i],\n",
    "        linestyle=styles[i],\n",
    "        label=labels[-1],\n",
    "        c=cmap.to_rgba(i + 3),\n",
    "    )[0]\n",
    "    lines.append(l)\n",
    "    plt.xticks([2, 4, 8, 16, 32, 64])\n",
    "    plt.xlabel(\"# Data parallel partitions\")\n",
    "    plt.ylabel(\"Throughput\\n(samples/second)\")\n",
    "leg = plt.figlegend(lines, labels, loc=\"upper center\", ncol=2)\n",
    "# Get the bounding box of the original legend.\n",
    "bb = leg.get_bbox_to_anchor().transformed(plt.gca().transAxes.inverted())\n",
    "\n",
    "# Change to location of the legend.\n",
    "yOffset = 0.2\n",
    "bb.y0 += yOffset\n",
    "bb.y1 += yOffset\n",
    "leg.set_bbox_to_anchor(bb, transform=plt.gca().transAxes)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data_parallelism.pdf\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num_devices = [2, 4, 8, 16, 32, 64]\n",
    "all_input_dims = [1024, 2048, 4096, 8192]\n",
    "batch_size = 8192\n",
    "num_hidden_layers = 128\n",
    "hp_results = defaultdict(list)\n",
    "for input_dim in all_input_dims:\n",
    "    hidden_dim = input_dim\n",
    "    output_dim = hidden_dim\n",
    "    topology = Topology()\n",
    "    d0 = topology.add_device(\"gpu\")\n",
    "    for i, num_devices in enumerate(all_num_devices):     \n",
    "        function = mlp(\n",
    "            batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, d0\n",
    "        )\n",
    "        function = infer_types(function, function.inputs)\n",
    "        if i == 0:\n",
    "            add_devices_to_topology(topology, num_devices)\n",
    "        else:\n",
    "            add_devices_to_topology(\n",
    "                topology, all_num_devices[i] - all_num_devices[i - 1]\n",
    "            )\n",
    "        assert len(topology.devices) == all_num_devices[i] + 1\n",
    "        simulator = Simulator(CostModel(topology))\n",
    "        transformed_function = parallel_transform_3d(\n",
    "            function,\n",
    "            1,\n",
    "            num_devices,\n",
    "            1,\n",
    "            topology.devices,\n",
    "            1,\n",
    "        )\n",
    "        transformed_function = infer_types(\n",
    "            transformed_function, transformed_function.inputs\n",
    "        )\n",
    "        transformed_function, typed_input_values = steady_state_transform(\n",
    "            transformed_function\n",
    "        )\n",
    "        transformed_function = infer_types(transformed_function, typed_input_values)\n",
    "        simulation = simulator.interpret(\n",
    "            transformed_function,\n",
    "            (v.type for v in transformed_function.inputs),\n",
    "        )\n",
    "        distributed_running_time = max(\n",
    "            [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "        )\n",
    "        hp_results[input_dim].append(batch_size / distributed_running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "markers = [\"o\", \"D\", \"v\", \"s\", \"<\", \"x\"]\n",
    "styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "c = np.arange(1, len(pp_results) + 3)\n",
    "norm = mpl.colors.Normalize(vmin=c.min(), vmax=c.max())\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.Blues)\n",
    "cmap.set_array([])\n",
    "plt.figure(figsize=(5, 3))\n",
    "lines = []\n",
    "labels = []\n",
    "for i, input_dim in enumerate(hp_results):\n",
    "    labels.append(f\"Weight dim {input_dim}\")\n",
    "    l = plt.plot(\n",
    "        all_num_devices,\n",
    "        hp_results[input_dim],\n",
    "        marker=markers[i],\n",
    "        linestyle=styles[i],\n",
    "        label=labels[-1],\n",
    "        c=cmap.to_rgba(i + 3),\n",
    "    )[0]\n",
    "    lines.append(l)\n",
    "    plt.xticks([2, 4, 8, 16, 32, 64])\n",
    "    plt.xlabel(\"# Horizontal parallel partitions\")\n",
    "    plt.ylabel(\"Throughput\\n(samples/second)\")\n",
    "leg = plt.figlegend(lines, labels, loc='upper center', ncol=2)\n",
    "# Get the bounding box of the original legend.\n",
    "bb = leg.get_bbox_to_anchor().transformed(plt.gca().transAxes.inverted())\n",
    "\n",
    "# Change to location of the legend. \n",
    "yOffset = 0.2\n",
    "bb.y0 += yOffset\n",
    "bb.y1 += yOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = plt.gca().transAxes)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"horizontal_parallelism.pdf\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num_devices = [2, 4, 8, 16, 32, 64]\n",
    "all_batch_sizes = [512, 1024, 2048, 4096]\n",
    "num_microbatches = 8\n",
    "input_dim = 8192\n",
    "hidden_dim = input_dim\n",
    "output_dim = hidden_dim\n",
    "batch_size = 8192\n",
    "num_hidden_layers = 64\n",
    "pp_results = defaultdict(list)\n",
    "for batch_size in all_batch_sizes:\n",
    "    topology = Topology()\n",
    "    d0 = topology.add_device(\"gpu\")\n",
    "    for i, num_devices in enumerate(all_num_devices):     \n",
    "        function = mlp(\n",
    "            batch_size, input_dim, hidden_dim, output_dim, num_hidden_layers, d0\n",
    "        )\n",
    "        function = infer_types(function, function.inputs)\n",
    "        if i == 0:\n",
    "            add_devices_to_topology(topology, num_devices)\n",
    "        else:\n",
    "            add_devices_to_topology(\n",
    "                topology, all_num_devices[i] - all_num_devices[i - 1]\n",
    "            )\n",
    "        assert len(topology.devices) == all_num_devices[i] + 1\n",
    "        simulator = Simulator(CostModel(topology))\n",
    "        transformed_function = parallel_transform_3d(\n",
    "            function,\n",
    "            1,\n",
    "            1,\n",
    "            num_devices,\n",
    "            topology.devices,\n",
    "            num_microbatches,\n",
    "        )\n",
    "        transformed_function = infer_types(\n",
    "            transformed_function, transformed_function.inputs\n",
    "        )\n",
    "        transformed_function, typed_input_values = steady_state_transform(\n",
    "            transformed_function\n",
    "        )\n",
    "        transformed_function = infer_types(transformed_function, typed_input_values)\n",
    "        simulation = simulator.interpret(\n",
    "            transformed_function,\n",
    "            (v.type for v in transformed_function.inputs),\n",
    "        )\n",
    "        distributed_running_time = max(\n",
    "            [simulation.timestamps[d] for d in simulation.timestamps]\n",
    "        )\n",
    "        pp_results[batch_size].append(batch_size / distributed_running_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "markers = [\"o\", \"D\", \"v\", \"s\", \"<\", \"x\"]\n",
    "styles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "c = np.arange(1, len(pp_results) + 3)\n",
    "norm = mpl.colors.Normalize(vmin=c.min(), vmax=c.max())\n",
    "cmap = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.Blues)\n",
    "cmap.set_array([])\n",
    "plt.figure(figsize=(5, 3))\n",
    "lines = []\n",
    "labels = []\n",
    "for i, batch_size in enumerate(pp_results):\n",
    "    labels.append(f\"Batch size {batch_size}\")\n",
    "    l = plt.plot(\n",
    "        all_num_devices,\n",
    "        pp_results[batch_size],\n",
    "        marker=markers[i],\n",
    "        label=labels[-1],\n",
    "        linestyle=styles[i],\n",
    "        c=cmap.to_rgba(i + 3)\n",
    "    )[0]\n",
    "    lines.append(l)\n",
    "    plt.xticks([2, 4, 8, 16, 32, 64])\n",
    "    plt.xlabel(\"# Pipeline parallel partitions\")\n",
    "    plt.ylabel(\"Throughput\\n(samples/second)\")\n",
    "leg = plt.figlegend(lines, labels, loc='upper center', ncol=2)\n",
    "# Get the bounding box of the original legend.\n",
    "bb = leg.get_bbox_to_anchor().transformed(plt.gca().transAxes.inverted())\n",
    "\n",
    "# Change to location of the legend. \n",
    "yOffset = 0.2\n",
    "bb.y0 += yOffset\n",
    "bb.y1 += yOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = plt.gca().transAxes)\n",
    "plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pipeline_parallelism.pdf\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-berry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
