{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(\"__file__\"))))\n",
    "from dist_ir.ir import FunctionMaker, Topology\n",
    "from dist_ir.ir.type import Tensor, Float\n",
    "from dist_ir.executor import infer_types, Simulator\n",
    "from dist_ir.executor.cost_model import CostModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_THROUGHPUT = 1.38e13 #6.7e12 # FLOPS\n",
    "DRAM_BANDWIDTH = 7e11 # ???\n",
    "PCIE_BANDWIDTH = 128 # Gbps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(world_size, batch_size, hidden_dim):\n",
    "    topology = Topology()\n",
    "    d0 = topology.add_device(\n",
    "        \"gpu\", throughput=DEVICE_THROUGHPUT, dram_bandwidth=DRAM_BANDWIDTH\n",
    "    )\n",
    "    for i in range(world_size):\n",
    "        di = topology.add_device(\n",
    "            \"gpu\", throughput=DEVICE_THROUGHPUT, dram_bandwidth=DRAM_BANDWIDTH\n",
    "        )\n",
    "        topology.set_bandwidth(d0, di, float(\"inf\"))\n",
    "        for j in range(1, i + 1):\n",
    "            dj = topology.devices[j]\n",
    "            topology.set_bandwidth(di, dj, PCIE_BANDWIDTH)\n",
    "    function = FunctionMaker()\n",
    "    x = function.add_input_value(\"x\", Tensor(dtype=Float(), shape=(batch_size, hidden_dim), device=d0))\n",
    "    w = function.add_input_value(\"w\", Tensor(dtype=Float(), shape=(hidden_dim, hidden_dim), device=d0))\n",
    "    if world_size == 1:\n",
    "        function.add_op(\"MatMul\", inputs=[x, w], output_names=[\"y\"])\n",
    "    else:\n",
    "        x1, x2 = function.add_op(\"MPIScatter\", inputs=[x],\n",
    "                                 attributes={\"dim\": 0, \"devices\": topology.devices[1:]},\n",
    "                                 output_names=[\"x1\", \"x2\"])\n",
    "        w1, w2 = function.add_op(\"MPIBroadcast\", inputs=[w],\n",
    "                                 attributes={\"devices\": topology.devices[1:]},\n",
    "                                 output_names=[\"w1\", \"w2\"])\n",
    "        y1 = function.add_op(\"MatMul\", inputs=[x1, w1], output_names=[\"y1\"])\n",
    "        y2 = function.add_op(\"MatMul\", inputs=[x2, w2], output_names=[\"y2\"])\n",
    "    function = function.finalize()\n",
    "    function = infer_types(function, function.inputs)\n",
    "    simulator = Simulator(CostModel(topology))\n",
    "    simulation = simulator.interpret(\n",
    "        function,\n",
    "        (v.type for v in function.inputs),\n",
    "    )\n",
    "    return max([simulation.timestamps[d] for d in simulation.timestamps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(\n",
    "    local_rank, world_size, backend=\"nccl\", master_addr=\"localhost\", master_port=\"12355\"\n",
    "):\n",
    "    os.environ[\"MASTER_ADDR\"] = master_addr\n",
    "    os.environ[\"MASTER_PORT\"] = master_port\n",
    "    torch.distributed.init_process_group(\n",
    "        backend, world_size=world_size, rank=local_rank\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    torch.distributed.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    (local_rank, world_size, batch_size, hidden_dim,\n",
    "     num_warmup_iterations, num_profiling_iterations) = config\n",
    "    # NOTE: I was previously splitting batch size manually here, but this appears to be incorrect\n",
    "    x = torch.randn((batch_size, hidden_dim)).cuda(local_rank)\n",
    "    model = torch.nn.Linear(hidden_dim, hidden_dim, bias=False).cuda(local_rank)\n",
    "    if world_size > 1:\n",
    "        setup(local_rank, world_size)\n",
    "        model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "    with profiler.profile(use_cuda=True) as prof:\n",
    "        with profiler.record_function(\"matmul_benchmark\"):\n",
    "            for i in range(num_warmup_iterations + num_profiling_iterations):\n",
    "                y = model(x)\n",
    "    if world_size > 1:\n",
    "        torch.distributed.barrier()\n",
    "    runtimes = []\n",
    "    for event in prof.function_events:\n",
    "        if event.name == \"aten::linear\":\n",
    "            assert event.cuda_time > 0\n",
    "            runtimes.append(event.cuda_time)\n",
    "\n",
    "    return np.median(runtimes[num_warmup_iterations:]) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_driver(batch_size, hidden_dim, world_size):\n",
    "    configs = [\n",
    "        (rank, world_size, batch_size, hidden_dim, 10, 100)\n",
    "        for rank in range(world_size)\n",
    "    ]\n",
    "    with torch.multiprocessing.Pool(world_size) as p:\n",
    "        results = p.map(run, configs)\n",
    "    print(\n",
    "        f\"world_size={world_size}, \"\n",
    "        f\"batch_size={batch_size}, \"\n",
    "        f\"hidden_dim={hidden_dim}, \"\n",
    "        f\"runtime={np.mean(results)}\"\n",
    "    )\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_benchmark():\n",
    "    all_batch_sizes = [256, 512, 1024]\n",
    "    all_hidden_dims = [2048, 4096]\n",
    "    simulated_throughputs = defaultdict(list)\n",
    "    pytorch_throughputs = defaultdict(list)\n",
    "    for batch_size in all_batch_sizes:\n",
    "        for hidden_dim in all_hidden_dims:\n",
    "            key = (batch_size, hidden_dim)\n",
    "            for world_size in [1, 2, 4]:\n",
    "                pytorch_runtime = distributed_driver(batch_size * world_size,\n",
    "                                                     hidden_dim, world_size)\n",
    "                simulated_runtime = simulate(world_size, batch_size * world_size, hidden_dim)\n",
    "                pytorch_throughputs[key].append(batch_size * world_size / pytorch_runtime / 1000)\n",
    "                simulated_throughputs[key].append(batch_size * world_size / simulated_runtime / 1000)\n",
    "    return pytorch_throughputs, simulated_throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world_size=1, batch_size=256, hidden_dim=2048, runtime=0.000249857421875\n",
      "world_size=2, batch_size=512, hidden_dim=2048, runtime=0.00024473583984375\n",
      "world_size=4, batch_size=1024, hidden_dim=2048, runtime=0.00025881591796875\n",
      "world_size=1, batch_size=256, hidden_dim=4096, runtime=0.000728578125\n",
      "world_size=2, batch_size=512, hidden_dim=4096, runtime=0.0007841276855468749\n",
      "world_size=4, batch_size=1024, hidden_dim=4096, runtime=0.0006968329467773438\n",
      "world_size=1, batch_size=512, hidden_dim=2048, runtime=0.00043827294921875\n",
      "world_size=2, batch_size=1024, hidden_dim=2048, runtime=0.00045209765624999997\n",
      "world_size=4, batch_size=2048, hidden_dim=2048, runtime=0.00038399554443359373\n",
      "world_size=1, batch_size=512, hidden_dim=4096, runtime=0.00135936328125\n",
      "world_size=2, batch_size=1024, hidden_dim=4096, runtime=0.00136780908203125\n",
      "world_size=4, batch_size=2048, hidden_dim=4096, runtime=0.0014635521240234375\n",
      "world_size=1, batch_size=1024, hidden_dim=2048, runtime=0.0007669765625\n",
      "world_size=2, batch_size=2048, hidden_dim=2048, runtime=0.0008064013671875\n",
      "world_size=4, batch_size=4096, hidden_dim=2048, runtime=0.0007109108886718751\n",
      "world_size=1, batch_size=1024, hidden_dim=4096, runtime=0.002716671875\n",
      "world_size=2, batch_size=2048, hidden_dim=4096, runtime=0.00283699267578125\n",
      "world_size=4, batch_size=4096, hidden_dim=4096, runtime=0.002635918212890625\n"
     ]
    }
   ],
   "source": [
    "(pytorch_throughputs, simulated_throughputs) = distributed_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 2048):\t[1024.58, 2092.05, 3956.48]\t[1402.14, 2804.28, 5608.56]\n",
      "(256, 4096):\t[351.37, 652.95, 1469.51]\t[353.43, 706.87, 1413.74]\n",
      "(512, 2048):\t[1168.22, 2265.00, 5333.40]\t[1500.64, 3001.27, 6002.54]\n",
      "(512, 4096):\t[376.65, 748.64, 1399.34]\t[378.48, 756.96, 1513.93]\n",
      "(1024, 2048):\t[1335.11, 2539.68, 5761.62]\t[1555.26, 3110.52, 6221.05]\n",
      "(1024, 4096):\t[376.93, 721.89, 1553.92]\t[392.39, 784.77, 1569.55]\n"
     ]
    }
   ],
   "source": [
    "for key in pytorch_throughputs:\n",
    "    print(f\"{str(key):10}:\\t[{pytorch_throughputs[key][0]:.2f}, {pytorch_throughputs[key][1]:.2f}, {pytorch_throughputs[key][2]:.2f}]\\t\"\n",
    "          f\"[{simulated_throughputs[key][0]:.2f}, {simulated_throughputs[key][1]:.2f}, {simulated_throughputs[key][2]:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_device_benchmark():\n",
    "    all_batch_sizes = [64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    all_hidden_dims = [64, 128, 256, 512, 1024, 2048, 4096]\n",
    "    simulation_results = defaultdict(list)\n",
    "    pytorch_results = defaultdict(list)\n",
    "    for batch_size in all_batch_sizes:\n",
    "        for hidden_dim in all_hidden_dims:\n",
    "            simulated_runtime = simulate(batch_size, hidden_dim)\n",
    "            pytorch_runtime = run(batch_size, hidden_dim)\n",
    "            simulation_results[batch_size].append(simulated_runtime)\n",
    "            pytorch_results[batch_size].append(pytorch_runtime)\n",
    "            print(f\"{batch_size},{hidden_dim},{simulated_runtime:.2f},{pytorch_runtime:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
